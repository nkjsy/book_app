{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25353, 2)\n",
      "                              title            author\n",
      "0                       The Martian         Andy Weir\n",
      "1  Career of Evil (Cormoran Strike)  Robert Galbraith\n",
      "2              The Crossing (Bosch)  Michael Connelly\n",
      "3           The Guilty (Will Robie)    David Baldacci\n",
      "4         Ready Player One: A Novel      Ernest Cline\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('book_list.csv',encoding = \"ISO-8859-1\", header=0)\n",
    "data.dropna(inplace=True)\n",
    "print (data.shape)\n",
    "print (data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n",
      "{'name', 'nine', 'she', 'each', 'and', 'whose', 'seems', 'who', 'hereupon', 'via', 'have', 'therefore', 'too', 'please', 'an', 'his', 'whence', 'serious', 'were', 'also', 'four', 'eight', 'between', 'whereupon', 'same', 'so', 'into', 'alone', 'though', 'else', 'nowhere', 'one', 'already', 'thereafter', 'yourself', 'without', 'side', 'any', 'anything', 'on', 'which', 'couldnt', 'etc', 'yourselves', 'such', 'although', 'become', 'your', 'nobody', 'sometimes', 'at', 'indeed', 'across', 'us', 'fill', 'get', 'they', 'being', 'whereafter', 'may', 'back', 'amoungst', 'these', 'off', 'mill', 'many', 'this', 'two', 'was', 'its', 'around', 'move', 'there', 'of', 'either', 'we', 'below', 'enough', 'however', 'anyone', 'rather', 'nor', 'sixty', 'describe', 'besides', 'hereafter', 'here', 'hereby', 'ourselves', 'namely', 'further', 'ltd', 'least', 'found', 'ie', 'meanwhile', 'beside', 'most', 'than', 'whither', 'eleven', 'ten', 'anywhere', 'them', 'my', 'now', 'very', 'can', 'our', 'with', 'call', 'within', 'thus', 'above', 'as', 'every', 'after', 'next', 'otherwise', 'con', 'less', 'ever', 'whatever', 'inc', 'becomes', 'then', 'should', 'might', 'upon', 'cannot', 'am', 'twenty', 'latter', 'thereby', 'three', 'once', 'whoever', 'that', 'the', 'against', 'but', 'nevertheless', 're', 'under', 'nothing', 'do', 'somehow', 'thru', 'where', 'up', 'latterly', 'elsewhere', 'some', 'by', 'whole', 'hence', 'formerly', 'those', 'while', 'seemed', 'go', 'only', 'through', 'forty', 'much', 'empty', 'himse\"', 'has', 'whereas', 'seem', 'twelve', 'give', 'five', 'afterwards', 'are', 'everything', 'cry', 'others', 'whether', 'show', 'had', 'thin', 'is', 'themselves', 'it', 'thick', 'eg', 'over', 'will', 'full', 'keep', 'amount', 'per', 'herein', 'fifteen', 'see', 'front', 'neither', 'to', 'both', 'whom', 'bill', 'several', 'whereby', 'him', 'top', 'because', 'during', 'becoming', 'moreover', 'third', 'other', 'wherein', 'another', 'yours', 'un', 'something', 'wherever', 'mine', 'own', 'be', 'always', 'a', 'out', 'anyway', 'even', 'detail', 'for', 'noone', 'yet', 'co', 'de', 'towards', 'well', 'toward', 'what', 'thereupon', 'everyone', 'he', 'myse\"', 'anyhow', 'still', 'ours', 'first', 'seeming', 'behind', 'hundred', 'from', 'fire', 'former', 'somewhere', 'could', 'six', 'until', 'everywhere', 'sometime', 'before', 'her', 'their', 'interest', 'became', 'hers', 'if', 'therein', 'throughout', 'when', 'thence', 'last', 'onto', 'put', 'i', 'system', 'all', 'down', 'often', 'computer', 'perhaps', 'bottom', 'among', 'in', 'amongst', 'not', 'beforehand', 'more', 'except', 'someone', 'fify', 'why', 'part', 'about', 'made', 'how', 'you', 'or', 'me', 'hasnt', 'cant', 'itse\"', 'together', 'never', 'find', 'herse\"', 'mostly', 'since', 'must', 'whenever', 'along', 'beyond', 'no', 'due', 'almost', 'take', 'would', 'none', 'done', 'few', 'again', 'been', 'sincere'}\n"
     ]
    }
   ],
   "source": [
    "sw_file = 'stopwords.txt'\n",
    "with open(sw_file, 'r') as fr:\n",
    "    stopwords = set(map(lambda x: x.strip(), fr.readlines()))\n",
    "print (len(stopwords))\n",
    "print (stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data cleaning\n",
    "\n",
    "### clean the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is english character?\n",
    "def is_valid(word):\n",
    "    for uchar in word:\n",
    "        if (uchar >= u'\\u0041' and uchar<=u'\\u005a') or (uchar >= u'\\u0061' and uchar<=u'\\u007a'):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion to clean the title column\n",
    "punc = '[~`\\!\\#\\$\\%^\\&\\*\\'\\(\\)_\\+\\-\\=\\|\\[\\]\\\\/\\:;\\.,\\?\\>\\<\\@\\\"\\{\\}]'\n",
    "def clean_title(line):\n",
    "    result = []\n",
    "    # remove the punctuation\n",
    "    line = re.sub(re.compile(punc), '', line)\n",
    "    words = line.strip().split()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        # exclude the stopwords or pure numbers\n",
    "        if word in stopwords or word.isdigit():\n",
    "            continue\n",
    "        # if not a valid english text, return a special text for future removal\n",
    "        if not is_valid(word):\n",
    "            return 'this is invalid'\n",
    "        result.append(word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_title = data['title'].map(clean_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the authors\n",
    "\n",
    "turn to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the author column\n",
    "def clean_author(line):\n",
    "    result = []\n",
    "    words = line.strip().split()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        # if not a valid english text, return a special text for future removal\n",
    "        if not is_valid(word):\n",
    "            return 'this is invalid'\n",
    "        result.append(word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_author = data['author'].map(clean_author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop the lines with invalid text and duplicate lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.DataFrame({'clean_title': col_title, 'clean_author': col_author, 'title': data['title'], 'author': data['author']})\n",
    "clean_df = clean_df[clean_df['clean_title'] != 'this is invalid']\n",
    "clean_df = clean_df[clean_df['clean_author'] != 'this is invalid']\n",
    "clean_df = clean_df.drop_duplicates(subset=['clean_title', 'clean_author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18416, 4)\n",
      "                   clean_title      clean_author  \\\n",
      "0                      martian         andy weir   \n",
      "1  career evil cormoran strike  robert galbraith   \n",
      "2               crossing bosch  michael connelly   \n",
      "3                 guilty robie    david baldacci   \n",
      "4           ready player novel      ernest cline   \n",
      "\n",
      "                              title            author  \n",
      "0                       The Martian         Andy Weir  \n",
      "1  Career of Evil (Cormoran Strike)  Robert Galbraith  \n",
      "2              The Crossing (Bosch)  Michael Connelly  \n",
      "3           The Guilty (Will Robie)    David Baldacci  \n",
      "4         Ready Player One: A Novel      Ernest Cline  \n"
     ]
    }
   ],
   "source": [
    "print (clean_df.shape)\n",
    "print (clean_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_author</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4788</th>\n",
       "      <td>alice wonderland edition norton critical editions</td>\n",
       "      <td>lewis carroll</td>\n",
       "      <td>Alice in Wonderland (Third Edition)  (Norton C...</td>\n",
       "      <td>Lewis Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22454</th>\n",
       "      <td>alices adventures wonderland popup adaptation</td>\n",
       "      <td>lewis carroll</td>\n",
       "      <td>Alice's Adventures in Wonderland: A Pop-up Ada...</td>\n",
       "      <td>Lewis Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_title   clean_author  \\\n",
       "4788   alice wonderland edition norton critical editions  lewis carroll   \n",
       "22454      alices adventures wonderland popup adaptation  lewis carroll   \n",
       "\n",
       "                                                   title         author  \n",
       "4788   Alice in Wonderland (Third Edition)  (Norton C...  Lewis Carroll  \n",
       "22454  Alice's Adventures in Wonderland: A Pop-up Ada...  Lewis Carroll  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df[clean_df['clean_author'] == 'lewis carroll']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concatenate the title and author together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_df = pd.DataFrame(clean_df['clean_title'] + ' ' + clean_df['clean_author'], columns=['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>martian andy weir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>career evil cormoran strike robert galbraith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>crossing bosch michael connelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>guilty robie david baldacci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ready player novel ernest cline</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            all\n",
       "0                             martian andy weir\n",
       "1  career evil cormoran strike robert galbraith\n",
       "2               crossing bosch michael connelly\n",
       "3                   guilty robie david baldacci\n",
       "4               ready player novel ernest cline"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# vectorize the books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SJ\\Anaconda3\\envs\\ml1\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1817: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. float32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18416, 12357)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, analyzer='word', token_pattern=r'\\w+', use_idf=True, smooth_idf=True, dtype='float32')\n",
    "vectors = vectorizer.fit_transform(one_df['all'])\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF vectorizer saved\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(vectorizer, '../model/vectorizer.pkl')\n",
    "print (\"TFIDF vectorizer saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18416, 30)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=30, learning_method='online')\n",
    "vectors_topic = lda.fit_transform(vectors)\n",
    "vectors_topic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model saved\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(lda, '../model/lda.pkl')\n",
    "print (\"LDA model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nearest neighbour model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = NearestNeighbors()\n",
    "knn.fit(vectors_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN model saved\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(knn, '../model/knn.pkl')\n",
    "print (\"KNN model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# books clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Mixture Model saved\n"
     ]
    }
   ],
   "source": [
    "gmm = GaussianMixture(n_components=8, n_init=3, max_iter=100, random_state=2020)\n",
    "gmm.fit(vectors_topic)\n",
    "joblib.dump(gmm, '../model/gmm.pkl')\n",
    "print (\"Gaussian Mixture Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = gmm.predict_proba(vectors_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the max groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to decide groups according to probability distribution?\n",
    "def into_groups(distribution):\n",
    "    max_p = np.max(distribution)\n",
    "    groups = []\n",
    "    if max_p > 0.9:\n",
    "        # if max proba > 0.9, return the max group\n",
    "        groups = [np.argmax(distribution)]\n",
    "    elif max_p < 0.4:\n",
    "        # if max proba < 0.4, not belong to any group, return a new group\n",
    "        groups = [len(distribution)]\n",
    "    else:\n",
    "        # if max proba is not dominant, return all the groups with proba > 0.1\n",
    "        ind = np.argsort(-distribution)\n",
    "        n_groups = len(distribution[distribution > 0.1])\n",
    "        groups = ind[:n_groups].tolist()\n",
    "    return ','.join(map(str, groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = []\n",
    "maxgroup = []\n",
    "for i in range(dist.shape[0]):\n",
    "    group = into_groups(dist[i,:])\n",
    "    groups.append(group)\n",
    "    maxgroup.append(group.split(',')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the original data for application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame({'title': clean_df['title'], 'author': clean_df['author'], 'groups': groups, 'maxgroup': maxgroup})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('archive.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
